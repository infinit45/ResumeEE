{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Neo/Projects\n",
      "/Users/Neo/opt/anaconda3/envs/myenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os, os.path\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "print(os.getcwd())\n",
    "print(sys.executable)\n",
    "\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha\\nApplication Development Associat...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina\\nHyderabad, Telangana - ...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anvitha Rao\\nAutomation developer\\n\\n- Email m...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 28...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arjun ks\\nSenior Program coordinator - oracle ...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 34...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arun Elumalai\\nQA Tester\\n\\nChennai, Tamil Nad...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 19...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ashalata Bisoyi\\nTransaction Processor - Oracl...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 17...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ashok Kunam\\nTeam Lead - Microsoft\\n\\n- Email ...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 41...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha\\nApplication Development Associat...   \n",
       "1  Afreen Jamadar\\nActive member of IIIT Committe...   \n",
       "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...   \n",
       "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...   \n",
       "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...   \n",
       "5  Anvitha Rao\\nAutomation developer\\n\\n- Email m...   \n",
       "6  arjun ks\\nSenior Program coordinator - oracle ...   \n",
       "7  Arun Elumalai\\nQA Tester\\n\\nChennai, Tamil Nad...   \n",
       "8  Ashalata Bisoyi\\nTransaction Processor - Oracl...   \n",
       "9  Ashok Kunam\\nTeam Lead - Microsoft\\n\\n- Email ...   \n",
       "\n",
       "                                          annotation  extras  \n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...     NaN  \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...     NaN  \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...     NaN  \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...     NaN  \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...     NaN  \n",
       "5  [{'label': ['Skills'], 'points': [{'start': 28...     NaN  \n",
       "6  [{'label': ['Skills'], 'points': [{'start': 34...     NaN  \n",
       "7  [{'label': ['Skills'], 'points': [{'start': 19...     NaN  \n",
       "8  [{'label': ['Skills'], 'points': [{'start': 17...     NaN  \n",
       "9  [{'label': ['Skills'], 'points': [{'start': 41...     NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "file = './ERR.json'\n",
    "\n",
    "df = pd.read_json(file, lines=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [{'label': ['Skills'], 'points': [{'start': 12...\n",
      "1      [{'label': ['Email Address'], 'points': [{'sta...\n",
      "2      [{'label': ['Skills'], 'points': [{'start': 37...\n",
      "3      [{'label': ['Skills'], 'points': [{'start': 80...\n",
      "4      [{'label': ['Degree'], 'points': [{'start': 20...\n",
      "                             ...                        \n",
      "215    [{'label': ['College Name'], 'points': [{'star...\n",
      "216    [{'label': ['Location'], 'points': [{'start': ...\n",
      "217    [{'label': ['Skills'], 'points': [{'start': 78...\n",
      "218    [{'label': ['Skills'], 'points': [{'start': 92...\n",
      "219    [{'label': ['Skills'], 'points': [{'start': 58...\n",
      "Name: annotation, Length: 220, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content'].replace(\"\\n\", \" \")\n",
    "            entities = []\n",
    "            data_annotations = data['annotation']\n",
    "            if data_annotations is not None:\n",
    "                for annotation in data_annotations:\n",
    "                    #only a single point in text annotation.\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "                    # handle both list of labels or a single label.\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "                        point_text = point['text']\n",
    "                        \n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start = point_start + lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end = point_end - rstrip_diff\n",
    "                        entities.append((point_start, point_end + 1 , label))\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\",\n",
       " {'entities': [[1296, 1622, 'Skills'],\n",
       "   [993, 1154, 'Skills'],\n",
       "   [939, 957, 'College Name'],\n",
       "   [883, 905, 'College Name'],\n",
       "   [856, 860, 'Graduation Year'],\n",
       "   [771, 814, 'College Name'],\n",
       "   [727, 769, 'Designation'],\n",
       "   [407, 416, 'Companies worked at'],\n",
       "   [372, 405, 'Designation'],\n",
       "   [95, 145, 'Email Address'],\n",
       "   [60, 69, 'Location'],\n",
       "   [49, 58, 'Companies worked at'],\n",
       "   [13, 46, 'Designation'],\n",
       "   [0, 12, 'Name']]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy(file))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import re\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing data to appropriate format so as to feed it to the model\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "cleanedDF = pd.DataFrame(columns=[\"setences_cleaned\"])\n",
    "sum1 = 0\n",
    "for i in range(len(data)):\n",
    "    start = 0\n",
    "    emptyList = [\"Empty\"] * len(data[i][0].split())\n",
    "    numberOfWords = 0\n",
    "    lenOfString = len(data[i][0])\n",
    "    strData = data[i][0]\n",
    "    strDictData = data[i][1]\n",
    "    lastIndexOfSpace = strData.rfind(' ')\n",
    "    for i in range(lenOfString):\n",
    "        if (strData[i]==\" \" and strData[i+1]!=\" \"):\n",
    "            for k,v in strDictData.items():\n",
    "                for j in range(len(v)):\n",
    "                    entList = v[len(v)-j-1]\n",
    "                    if (start>=int(entList[0]) and i<=int(entList[1])):\n",
    "                        emptyList[numberOfWords] = entList[2]\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "            start = i + 1  \n",
    "            numberOfWords += 1\n",
    "        if (i == lastIndexOfSpace):\n",
    "            for j in range(len(v)):\n",
    "                    entList = v[len(v)-j-1]\n",
    "                    if (lastIndexOfSpace>=int(entList[0]) and lenOfString<=int(entList[1])):\n",
    "                        emptyList[numberOfWords] = entList[2]\n",
    "                        numberOfWords += 1\n",
    "    cleanedDF = cleanedDF.append(pd.Series([emptyList],  index=cleanedDF.columns ), ignore_index=True )\n",
    "    sum1 = sum1 + numberOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANk0lEQVR4nO3dbaxlZ1nG8f9lp6W82ik9kNoSz9Q0SmMUmkmtYvhACS+tsTUpSY3RCTZpoqAgGhkkEfzWGgU1IZCRYkZDoLVg2tgoNqWN8YODp6XQlrHOUGopHTuHQHnxg1C5/bCfKScz52Wftzn7xv8vOdlrPWvtWfc9a8816zx7r3NSVUiS+vqhnS5AkrQ5BrkkNWeQS1JzBrkkNWeQS1Jzu07nwc4777yan58/nYeUpPbuu+++r1bV3ErbT2uQz8/Ps7CwcDoPKUntJfnP1bY7tSJJzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzZ3WOzs3Y37/nTty3MduvGpHjitJ0/KKXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKamyrIk/xOkoeTPJTkY0nOTrInyaEkR5LckuSs7S5WknSqNYM8yQXAbwN7q+ongTOA64CbgPdX1cXA14Hrt7NQSdLypp1a2QU8N8ku4HnAMeA1wG1j+0Hgmq0vT5K0ljWDvKq+AvwJ8DiTAP8GcB/wdFU9M3Z7ArhguecnuSHJQpKFxcXFralakvSsaaZWdgNXA3uAHwGeD7xxmV1ruedX1YGq2ltVe+fm5jZTqyRpGdNMrbwW+FJVLVbVd4FPAj8HnDOmWgAuBJ7cpholSauYJsgfBy5P8rwkAa4AvgDcA1w79tkH3L49JUqSVjPNHPkhJm9q3g88OJ5zAHgn8I4kR4EXAzdvY52SpBXsWnsXqKr3AO85afhR4LItr0iStC7e2SlJzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5JzRnkktScQS5Jze3a6QJm3fz+O3fs2I/deNWOHVtSH16RS1JzBrkkNWeQS1JzBrkkNWeQS1JzBrkkNWeQS1JzBrkkNTdVkCc5J8ltSf49yeEkP5vk3CR3JTkyHndvd7GSpFNNe0X+58A/VtVPAD8NHAb2A3dX1cXA3WNdknSarRnkSV4EvBq4GaCqvlNVTwNXAwfHbgeBa7arSEnSyqa5Ir8IWAT+Kslnk3w4yfOBl1bVMYDx+JLlnpzkhiQLSRYWFxe3rHBJ0sQ0Qb4LuBT4YFW9Evhv1jGNUlUHqmpvVe2dm5vbYJmSpJVME+RPAE9U1aGxfhuTYH8qyfkA4/H49pQoSVrNmkFeVf8FfDnJj4+hK4AvAHcA+8bYPuD2balQkrSqaX8e+W8BH01yFvAo8GYm/wncmuR64HHgTdtToiRpNVMFeVU9AOxdZtMVW1uOJGm9vLNTkpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpuamDPMkZST6b5O/H+p4kh5IcSXJLkrO2r0xJ0krWc0X+NuDwkvWbgPdX1cXA14Hrt7IwSdJ0pgryJBcCVwEfHusBXgPcNnY5CFyzHQVKklY37RX5nwG/D3xvrL8YeLqqnhnrTwAXLPfEJDckWUiysLi4uKliJUmnWjPIk/wCcLyq7ls6vMyutdzzq+pAVe2tqr1zc3MbLFOStJJdU+zzKuAXk1wJnA28iMkV+jlJdo2r8guBJ7evTEnSSta8Iq+qd1XVhVU1D1wHfLqqfgW4B7h27LYPuH3bqpQkrWgznyN/J/COJEeZzJnfvDUlSZLWY5qplWdV1b3AvWP5UeCyrS9JkrQe3tkpSc0Z5JLUnEEuSc0Z5JLUnEEuSc0Z5JLUnEEuSc0Z5JLUnEEuSc0Z5JLUnEEuSc0Z5JLUnEEuSc0Z5JLUnEEuSc0Z5JLUnEEuSc0Z5JLUnEEuSc2t63d26vSa33/njhz3sRuv2pHjStoYr8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaM8glqTmDXJKaWzPIk7wsyT1JDid5OMnbxvi5Se5KcmQ87t7+ciVJJ5vmivwZ4Her6uXA5cBbklwC7AfurqqLgbvHuiTpNFszyKvqWFXdP5a/BRwGLgCuBg6O3Q4C12xXkZKkla1rjjzJPPBK4BDw0qo6BpOwB16ywnNuSLKQZGFxcXFz1UqSTjF1kCd5AfAJ4O1V9c1pn1dVB6pqb1XtnZub20iNkqRVTBXkSc5kEuIfrapPjuGnkpw/tp8PHN+eEiVJq5nmUysBbgYOV9X7lmy6A9g3lvcBt299eZKkteyaYp9XAb8KPJjkgTH2B8CNwK1JrgceB960PSVKklazZpBX1b8AWWHzFVtbjiRpvbyzU5KaM8glqTmDXJKam+bNTv0/M7//zh079mM3XrVjx5a68opckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckpozyCWpOYNckprzNwRppuzUbyfyNxOpM6/IJak5g1ySmjPIJak5g1ySmjPIJak5g1ySmjPIJak5g1ySmvOGIGmHeROUNssrcklqziCXpOYMcklqzjlyiZ2bp95JO9nzTs3P/6C+H+EVuSQ1t6kgT/KGJI8kOZpk/1YVJUma3oaDPMkZwAeANwKXAL+c5JKtKkySNJ3NXJFfBhytqker6jvAx4Grt6YsSdK0NvNm5wXAl5esPwH8zMk7JbkBuGGsfjvJI+s4xnnAVzdc4eyxn9lmP6dJbtrQ02a2n7Ws0O96+vnR1TZuJsizzFidMlB1ADiwoQMkC1W1dyPPnUX2M9vsZ7bZz8o2M7XyBPCyJesXAk9urhxJ0nptJsj/Dbg4yZ4kZwHXAXdsTVmSpGlteGqlqp5J8lbgU8AZwEeq6uEtq2xiQ1MyM8x+Zpv9zDb7WUGqTpnWliQ14p2dktScQS5Jzc1skHe8/T/JY0keTPJAkoUxdm6Su5IcGY+7x3iS/MXo7/NJLt3Z6ieSfCTJ8SQPLRlbdw9J9o39jyTZtxO9jDqW6+e9Sb4yztMDSa5csu1do59Hkrx+yfiOvx6TvCzJPUkOJ3k4ydvGeMvzs0o/Lc/PqOPsJJ9J8rnR0x+N8T1JDo2/71vGB0RI8pyxfnRsn1/yZy3b67Kqaua+mLx5+kXgIuAs4HPAJTtd1xR1Pwacd9LYHwP7x/J+4KaxfCXwD0w+j385cGin6x91vRq4FHhooz0A5wKPjsfdY3n3DPXzXuD3ltn3kvFaew6wZ7wGz5iV1yNwPnDpWH4h8B+j5pbnZ5V+Wp6fUWOAF4zlM4FD4+/+VuC6Mf4h4DfG8m8CHxrL1wG3rNbrSsed1SvyH6Tb/68GDo7lg8A1S8b/uib+FTgnyfk7UeBSVfXPwNdOGl5vD68H7qqqr1XV14G7gDdsf/WnWqGflVwNfLyq/qeqvgQcZfJanInXY1Udq6r7x/K3gMNM7rBueX5W6WclM31+AMbf9bfH6pnjq4DXALeN8ZPP0YlzdxtwRZKwcq/LmtUgX+72/9VO8Kwo4J+S3JfJjyYAeGlVHYPJCxd4yRjv1ON6e+jQ21vHdMNHTkxF0Kif8S34K5lc8bU/Pyf1A43PT5IzkjwAHGfyn+QXgaer6pll6nu29rH9G8CLWWdPsxrkU93+P4NeVVWXMvmJkG9J8upV9u3a41Ir9TDrvX0Q+DHgFcAx4E/HeIt+krwA+ATw9qr65mq7LjPWoZ/W56eq/reqXsHkbvfLgJcvt9t43JKeZjXIW97+X1VPjsfjwN8xOYlPnZgyGY/Hx+6delxvDzPdW1U9Nf6xfQ/4S77/LevM95PkTCah99Gq+uQYbnt+luun8/lZqqqeBu5lMkd+TpITN2Aure/Z2sf2H2YyFbiunmY1yNvd/p/k+UleeGIZeB3wEJO6T3wqYB9w+1i+A/i18cmCy4FvnPj2eAatt4dPAa9Lsnt8W/y6MTYTTnov4peYnCeY9HPd+CTBHuBi4DPMyOtxzJ3eDByuqvct2dTy/KzUT9fzA5BkLsk5Y/m5wGuZzP3fA1w7djv5HJ04d9cCn67Ju50r9bq8nXhnd8p3f69k8i72F4F373Q9U9R7EZN3mT8HPHyiZibzXXcDR8bjufX9d7c/MPp7ENi70z2Muj7G5NvZ7zK5Krh+Iz0Av87kDZqjwJtnrJ+/GfV+fvyDOX/J/u8e/TwCvHGWXo/AzzP59vrzwAPj68qu52eVflqen1HHTwGfHbU/BPzhGL+ISRAfBf4WeM4YP3usHx3bL1qr1+W+vEVfkpqb1akVSdKUDHJJas4gl6TmDHJJas4gl6TmDHJJas4gl6Tm/g8IKxKE4IzUtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "totalNumWords = [len(one_comment.split()) for one_comment in df[\"content\"]]\n",
    "plt.hist(totalNumWords)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import re\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "bs = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "#n_gpu = torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(df[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3331, 3332, 37, 52, 302, 517, 150, 72, 62, 59, 8, 11, 11, 13, 17, 3331, 3332, 4387, 2, 4, 23, 7, 88, 193, 105, 603, 59, 3, 890, 4, 814, 120, 20, 1, 58, 7, 120, 843, 1, 2736, 448, 5, 341, 1354, 2079, 190, 4, 198, 4, 154, 72, 23, 16, 37, 52, 302, 517, 320, 111, 4, 91, 163, 534, 76, 8, 1733, 2737, 289, 1891, 39, 1734, 449, 7, 3, 2737, 105, 244, 139, 3333, 97, 8, 481, 1355, 221, 130, 3, 2737, 7, 172, 1354, 6000, 408, 786, 1, 3334, 105, 244, 139, 481, 15, 1355, 41, 3, 128, 63, 143, 234, 5, 54, 241, 1, 145, 143, 535, 143, 159, 6, 145, 1, 78, 3335, 72, 257, 110, 4, 131, 111, 2738, 5, 2739, 6001, 3336, 202, 283, 175, 4, 290, 110, 2381, 6002, 6003, 283, 1263, 4, 290, 175, 20, 73, 67, 56, 24, 33, 83, 67, 56, 24, 33, 83, 12, 67, 56, 24, 33, 83, 12, 44, 67, 56, 24, 33, 81, 67, 56, 24, 33, 100, 54, 45, 20, 32, 34, 11, 13, 17, 3331, 3332, 4387, 47, 48, 14, 49, 14, 43, 35, 5, 2, 267, 329, 73, 73, 81, 2, 39, 1734, 2, 815, 6, 2382, 2, 891, 426, 2, 83, 12, 44, 2, 170, 1086, 2, 192, 44, 66, 8, 188, 71, 1356, 671, 45, 20, 2, 2383, 1, 892, 76, 2, 6004, 1, 965, 4, 172, 1087, 2, 6005, 1, 4388, 2, 19, 518]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = tokenizer.texts_to_sequences(df[\"content\"])\n",
    "# tokenized_texts = [tokenizer.tokenize(sent) for sent in df_data[\"content\"]]\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(tokenized_texts,\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = [\"UNKNOWN\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Empty\",\"Graduation Year\",\"Years of Experience\",\"Location\"]\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cleanedDF[\"setences_cleaned\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'College Name', 'Years of Experience', 'Name', 'Degree', 'UNKNOWN', 'Empty', 'Companies worked at', 'Skills', 'Designation'}\n"
     ]
    }
   ],
   "source": [
    "print(set(labels[61]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"Empty\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
